version: "3.7"

services:
    db:
        image: postgis/postgis:13-master
        environment:
            - POSTGRES_USER=postgres
            - POSTGRES_MULTIPLE_DATABASES=configdb,observationportal,downtime,sciencearchive
            - POSTGRES_PASSWORD=postgrespass
        volumes:
            - ./docker-postgresql-multiple-databases:/docker-entrypoint-initdb.d
        mem_limit: "2048m"
        healthcheck:
            test: ["CMD-SHELL", "pg_isready -U postgres"]
            interval: 10s
            timeout: 5s
            retries: 5
        restart: always

    configdb:
        image: observatorycontrolsystem/configdb:2.1.1
        ports:
            - "7000:7000"
        environment:
            - DB_HOST=db
            - DB_NAME=configdb
            - DB_USER=postgres
            - DB_PASS=postgrespass
            - SECRET_KEY=ocs_example_configdb_secret_key
            - DEBUG=true
            - OAUTH_CLIENT_ID=configdb_application_client_id
            - OAUTH_CLIENT_SECRET=configdb_application_client_secret
            - OAUTH_TOKEN_URL=http://observation-portal:8000/o/token/
        mem_limit: "512m"
        restart: always
        healthcheck:
            test: ["CMD-SHELL", "wget localhost:7000/genericmodes/ -q -O - > /dev/null 2>&1"]
            interval: 10s
            timeout: 5s
            retries: 5
        command: >
            bash -c "python manage.py migrate 
            && python manage.py init_e2e_data -s ogg --latitude 20.707 --longitude -156.258 --instrument-state=SCHEDULABLE
            && python manage.py runserver 0.0.0.0:7000"
        depends_on:
            db:
                condition: service_healthy

    downtime:
        image: observatorycontrolsystem/downtime:2.3.2
        ports:
            - "7500:7500"
        environment:
            - DB_ENGINE=django.db.backends.postgresql
            - DB_HOST=db
            - DB_NAME=downtime
            - DB_USER=postgres
            - DB_PASS=postgrespass
            - SECRET_KEY=ocs_example_downtime_secret_key
            - CONFIGDB_URL=http://configdb:7000
            - OAUTH_CLIENT_ID=downtime_application_client_id
            - OAUTH_CLIENT_SECRET=downtime_application_client_secret
            - OAUTH_TOKEN_URL=http://observation-portal:8000/o/token/
            - OAUTH_PROFILE_URL=http://observation-portal:8000/api/profile/
        mem_limit: "512m"
        restart: always
        command: >
            sh -c "python manage.py migrate
            && python manage.py shell -c \"from django.contrib.auth.models import User; User.objects.create_superuser('test_user', 'test_user@fake.com', 'test_pass')\"
            && python manage.py create_downtime -s ogg -e doma -t 1m0a -r Weather --offset-hours=24 --duration-hours=24
            && python manage.py create_downtime -s ogg -e clma -t 2m0a -r Maintenance --offset-hours=-48 --duration-hours=24
            && python manage.py runserver 0.0.0.0:7500"
        depends_on:
            db:
                condition: service_healthy
            configdb:
                condition: service_healthy

    observation-portal:
        image: observatorycontrolsystem/observation-portal:3.1.13
        ports:
            - "8000:8000"
        environment:
            - DB_HOST=db
            - DB_NAME=observationportal
            - DB_USER=postgres
            - DB_PASSWORD=postgrespass
            - DEBUG=true
            - SECRET_KEY=ocs_example_obs_portal_secret_key
            - CONFIGDB_URL=http://configdb:7000
            - DOWNTIMEDB_URL=http://downtime:7500
            - ELASTICSEARCH_URL=
            - CORS_ALLOW_CREDENTIALS=true
            - CORS_ORIGIN_WHITELIST=http://localhost:8080,http://127.0.0.1:8080
            - CSRF_TRUSTED_ORIGINS=localhost:8080,127.0.0.1:8080
            - DRAMATIQ_BROKER_HOST=redis
            - DRAMATIQ_BROKER_PORT=6379
        mem_limit: "512m"
        restart: always
        command: >
            sh -c "python manage.py migrate 
            && python manage.py create_user -u test_user -p test_pass --superuser --token=sutoken1234abcd
            && python manage.py create_application -u test_user -n ConfigDB --client-id configdb_application_client_id --client-secret configdb_application_client_secret --redirect-uris http://localhost:7000
            && python manage.py create_application -u test_user -n Downtime --client-id downtime_application_client_id --client-secret downtime_application_client_secret --redirect-uris http://localhost:7500/
            && python manage.py create_application -u test_user -n Archive --client-id science_archive_application_client_id --client-secret science_archive_application_client_secret --redirect-uris http://localhost:9500/
            && python manage.py create_semester --id TestSemester
            && python manage.py create_proposal --id TestProposal --active --direct --pi test_user --time-allocation
            && python manage.py create_example_requests -p TestProposal -s test_user
            && python manage.py collectstatic --no-input
            && python manage.py runserver 0.0.0.0:8000"
        depends_on:
            db:
                condition: service_healthy
            configdb:
                condition: service_healthy

    dramatiq_task_scheduler:
        image: observatorycontrolsystem/observation-portal:3.1.13
        environment:
            - DB_HOST=db
            - DB_NAME=observationportal
            - DB_USER=postgres
            - DB_PASSWORD=postgrespass
            - DEBUG=true
            - SECRET_KEY=ocs_example_obs_portal_secret_key
            - CONFIGDB_URL=http://configdb:7000
            - DOWNTIMEDB_URL=http://downtime:7500
            - ELASTICSEARCH_URL=
            - CORS_ALLOW_CREDENTIALS=true
            - CORS_ORIGIN_WHITELIST=http://localhost:8080,http://127.0.0.1:8080
            - CSRF_TRUSTED_ORIGINS=localhost:8080,127.0.0.1:8080
            - DRAMATIQ_BROKER_HOST=redis
            - DRAMATIQ_BROKER_PORT=6379
        mem_limit: "512m"
        restart: always
        command: >
            sh -c "python manage.py runscript observation_portal.task_scheduler"
        depends_on:
            db:
                condition: service_healthy
            configdb:
                condition: service_healthy
            redis:
                condition: service_healthy

    dramatiq_worker:
        image: observatorycontrolsystem/observation-portal:3.1.13
        environment:
            - DB_HOST=db
            - DB_NAME=observationportal
            - DB_USER=postgres
            - DB_PASSWORD=postgrespass
            - DEBUG=true
            - SECRET_KEY=ocs_example_obs_portal_secret_key
            - CONFIGDB_URL=http://configdb:7000
            - DOWNTIMEDB_URL=http://downtime:7500
            - ELASTICSEARCH_URL=
            - CORS_ALLOW_CREDENTIALS=true
            - CORS_ORIGIN_WHITELIST=http://localhost:8080,http://127.0.0.1:8080
            - CSRF_TRUSTED_ORIGINS=localhost:8080,127.0.0.1:8080
            - DRAMATIQ_BROKER_HOST=redis
            - DRAMATIQ_BROKER_PORT=6379
        mem_limit: "512m"
        restart: always
        command: >
            sh -c "python manage.py rundramatiq --processes 2 --threads 4"
        depends_on:
            db:
                condition: service_healthy
            configdb:
                condition: service_healthy
            redis:
                condition: service_healthy

    adaptive_scheduler:
        image: observatorycontrolsystem/adaptive_scheduler:1.1.10
        restart: always
        links:
            -  redis:redis
        environment:
            -  OPENTSDB_HOSTNAME=opentsdb-path
            -  OPENTSDB_PYTHON_METRICS_TEST_MODE=True
            -  OBSERVATION_PORTAL_URL=http://observation-portal:8000
            -  OBSERVATION_PORTAL_API_TOKEN=sutoken1234abcd
            -  CONFIGDB_URL=http://configdb:7000
            -  DOWNTIMEDB_URL=http://downtime:7500/
            -  REDIS_URL=redis
            -  TELESCOPE_CLASSES=1m0,2m0
            -  SAVE_PICKLE_INPUT_FILES=False
            -  SAVE_JSON_OUTPUT_FILES=False
            -  TIME_BETWEEN_RUNS=300
            -  KERNEL_TIMELIMIT=1200
            -  MODEL_HORIZON=2
            -  MODEL_SLICESIZE=300
            -  NO_WEATHER=True
            -  KERNEL_ALGORITHM=SCIP
        volumes:
            # Edit these volume maps to wherever you want to store the log files and input/output data sets
            -  ./data/input:/data/adaptive_scheduler/input_states/
            -  ./data/output:/data/adaptive_scheduler/output_schedule/
            -  ./data/:/data/adaptive_scheduler/
            -  ./logs/:/ocs/adaptive_scheduler/logs/
        working_dir: /ocs/adaptive_scheduler/
        command: python as.py
        depends_on:
            db:
                condition: service_healthy
            configdb:
                condition: service_healthy
            redis:
                condition: service_healthy

    ocs_frontend:
        image: observatorycontrolsystem/ocs-example-frontend:0.1.0
        ports:
            - "8080:8080"
        restart: always
        environment:
            -  VUE_APP_OBSERVATION_PORTAL_API_URL=http://localhost:8000
            -  INTERNAL_OBSERVATION_PORTAL_API_URL=http://localhost:8000
        entrypoint: /entrypoint.sh

    # The reason for using four minio servers:
    # The science archive requires an "s3" bucket with versioning enabled.
    # In order to use versioning, minio requires a distributed erasure code setup.
    # Distributed minio setups require at least four servers.
    # For details, see: https://docs.min.io/docs/minio-erasure-code-quickstart-guide.html
    minio1:
        image: minio/minio:RELEASE.2021-06-17T00-10-46Z
        volumes:
            - data1-1:/data1
            - data1-2:/data2
        expose:
            - "9000"
        environment:
            MINIO_ROOT_USER: minio_access_key
            MINIO_ROOT_PASSWORD: minio_secret
            MINIO_REGION_NAME: minio-region
        command: server http://minio{1...4}/data{1...2}
        healthcheck:
            test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
            interval: 30s
            timeout: 20s
            retries: 3
    minio2:
        image: minio/minio:RELEASE.2021-06-17T00-10-46Z
        volumes:
            - data2-1:/data1
            - data2-2:/data2
        expose:
            - "9000"
        environment:
            MINIO_ROOT_USER: minio_access_key
            MINIO_ROOT_PASSWORD: minio_secret
            MINIO_REGION_NAME: minio-region
        command: server http://minio{1...4}/data{1...2}
        healthcheck:
            test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
            interval: 30s
            timeout: 20s
            retries: 3
    minio3:
        image: minio/minio:RELEASE.2021-06-17T00-10-46Z
        volumes:
            - data3-1:/data1
            - data3-2:/data2
        expose:
            - "9000"
        environment:
            MINIO_ROOT_USER: minio_access_key
            MINIO_ROOT_PASSWORD: minio_secret
            MINIO_REGION_NAME: minio-region
        command: server http://minio{1...4}/data{1...2}
        healthcheck:
            test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
            interval: 30s
            timeout: 20s
            retries: 3
    minio4:
        image: minio/minio:RELEASE.2021-06-17T00-10-46Z
        volumes:
            - data4-1:/data1
            - data4-2:/data2
        expose:
            - "9000"
        environment:
            MINIO_ROOT_USER: minio_access_key
            MINIO_ROOT_PASSWORD: minio_secret
            MINIO_REGION_NAME: minio-region
        command: server http://minio{1...4}/data{1...2}
        healthcheck:
            test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
            interval: 30s
            timeout: 20s
            retries: 3

    nginx:
        image: nginx:1.19.2-alpine
        volumes:
        - ./nginx.conf:/etc/nginx/nginx.conf:ro
        network_mode: "service:science_archive"
        depends_on:
        - minio1
        - minio2
        - minio3
        - minio4

    science_archive:
        image: observatorycontrolsystem/science-archive:1.78
        container_name: sciencearchive
        ports:
            - "9000:9000"  # minio
            - "9500:9500"  # archive api
        environment:
            - DEBUG=True  # must be true to load static files
            - DB_HOST=db
            - DB_HOST_READER=db
            - DB_NAME=sciencearchive
            - DB_USER=postgres
            - DB_PASS=postgrespass
            - SECRET_KEY=ocs_example_science_archive_secret_key
            - AWS_ACCESS_KEY_ID=minio_access_key
            - AWS_SECRET_ACCESS_KEY=minio_secret
            - AWS_DEFAULT_REGION=minio-region
            - AWS_BUCKET=ocs-example-bucket
            - S3_ENDPOINT_URL=http://localhost:9000/
            - OAUTH_CLIENT_ID=science_archive_application_client_id
            - OAUTH_CLIENT_SECRET=science_archive_application_client_secret
            - OAUTH_TOKEN_URL=http://observation-portal:8000/o/token/
            - OAUTH_PROFILE_URL=http://observation-portal:8000/api/profile/
            - PROCESSED_EXCHANGE_ENABLED=False
            # Additional variables for the ingester:
            - API_ROOT=http://localhost:9500/
            - OBSERVATION_PORTAL_BASE_URL=http://observation-portal:8000/api
            - BUCKET=ocs-example-bucket
            - OPENTSDB_PYTHON_METRICS_TEST_MODE=True
        mem_limit: "512m"
        restart: always
        volumes: 
            - ./example_data:/example_data
            - ./scripts/science_archive:/scripts
        healthcheck:
            test: ["CMD-SHELL", "wget localhost:9500/ -q -O - > /dev/null 2>&1"]
            interval: 10s
            timeout: 5s
            retries: 5
        command: >
            sh -c "python manage.py migrate ;
            python manage.py collectstatic --no-input ;
            python manage.py runserver 0.0.0.0:9500 &
            pip install ocs_ingester requests ;
            export AUTH_TOKEN=$$( echo 'from django.contrib.auth.models import User; User.objects.create_superuser(\"test_user\", \"admin@example.com\", \"test_pass\"); print(User.objects.first().auth_token)' | python manage.py shell);
            echo $$AUTH_TOKEN ;
            python /scripts/initialize_bucket.py ;
            python /scripts/ingest_sample_data.py ;
            wait %1"
        depends_on:
        - observation-portal
        - minio1
        - minio2
        - minio3
        - minio4

    redis:
        image: redis:3.2
        command: ["redis-server", "--appendonly", "yes"]
        restart: always
        ports:
            -  6373:6379
        volumes:
            -  ./data/redis:/data
        healthcheck:
            test: ["CMD", "redis-cli", "ping"]
            interval: 10s
            timeout: 5s
            retries: 30

    virtual-site:
        image: ghcr.io/observatorycontrolsystem/virtual-site:latest
        restart: always
        command: >
            --name=ogg --api-url=http://observation-portal:8000/api
            --api-token='sutoken1234abcd'
        depends_on:
          - observation-portal

## By default this config uses default local driver,
## For custom volumes replace with volume driver configuration.
volumes:
    data1-1:
    data1-2:
    data2-1:
    data2-2:
    data3-1:
    data3-2:
    data4-1:
    data4-2:
