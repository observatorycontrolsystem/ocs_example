version: "3.7"

services:
    db:
        image: postgis/postgis:13-master
        platform: linux/x86_64
        environment:
            - POSTGRES_USER=postgres
            - POSTGRES_MULTIPLE_DATABASES=configdb,observationportal,downtime,sciencearchive
            - POSTGRES_PASSWORD=postgrespass
        volumes:
            - ./docker-postgresql-multiple-databases:/docker-entrypoint-initdb.d
        mem_limit: "2048m"
        healthcheck:
            test: ["CMD-SHELL", "pg_isready -U postgres"]
            interval: 10s
            timeout: 5s
            retries: 5
        restart: always

    configdb:
        image: ghcr.io/observatorycontrolsystem/configdb:3.0.4
        platform: linux/x86_64
        ports:
            - "7000:7000"
        environment:
            - DB_HOST=db
            - DB_NAME=configdb
            - DB_USER=postgres
            - DB_PASS=postgrespass
            - SECRET_KEY=ocs_example_configdb_secret_key
            - DEBUG=true
            - OAUTH_CLIENT_ID=configdb_application_client_id
            - OAUTH_CLIENT_SECRET=configdb_application_client_secret
            - OAUTH_TOKEN_URL=http://observation-portal:8000/o/token/
            - OAUTH_PROFILE_URL=http://observation-portal:8000/api/profile/
            - OAUTH_SERVER_KEY=mysecretserverkey12345
        mem_limit: "512m"
        restart: always
        healthcheck:
            test: ["CMD-SHELL", "wget localhost:7000/genericmodes/ -q -O - > /dev/null 2>&1"]
            interval: 10s
            timeout: 5s
            retries: 5
        command: >
            bash -c "python manage.py migrate 
            && python manage.py init_e2e_data -s ogg --latitude 20.707 --longitude -156.258 --instrument-state=SCHEDULABLE
            && python manage.py runserver 0.0.0.0:7000"
        depends_on:
            db:
                condition: service_healthy

    downtime:
        image: ghcr.io/observatorycontrolsystem/downtime:2.4.2
        platform: linux/x86_64
        ports:
            - "7500:7500"
        environment:
            - DB_ENGINE=django.db.backends.postgresql
            - DB_HOST=db
            - DB_NAME=downtime
            - DB_USER=postgres
            - DB_PASS=postgrespass
            - SECRET_KEY=ocs_example_downtime_secret_key
            - CONFIGDB_URL=http://configdb:7000
            - OAUTH_CLIENT_ID=downtime_application_client_id
            - OAUTH_CLIENT_SECRET=downtime_application_client_secret
            - OAUTH_TOKEN_URL=http://observation-portal:8000/o/token/
            - OAUTH_PROFILE_URL=http://observation-portal:8000/api/profile/
            - OAUTH_SERVER_KEY=mysecretserverkey12345
        mem_limit: "512m"
        restart: always
        command: >
            sh -c "python manage.py migrate
            && python manage.py create_downtime -s ogg -e doma -t 1m0a -r Weather --offset-hours=24 --duration-hours=24
            && python manage.py create_downtime -s ogg -e clma -t 2m0a -r Maintenance --offset-hours=-48 --duration-hours=24
            && python manage.py runserver 0.0.0.0:7500"
        depends_on:
            db:
                condition: service_healthy
            configdb:
                condition: service_healthy

    observation-portal:
        image: ghcr.io/observatorycontrolsystem/observation-portal:4.8.0
        platform: linux/x86_64
        ports:
            - "8000:8000"
        environment:
            - DB_HOST=db
            - DB_NAME=observationportal
            - DB_USER=postgres
            - DB_PASSWORD=postgrespass
            - DEBUG=true
            - SECRET_KEY=ocs_example_obs_portal_secret_key
            - CONFIGDB_URL=http://configdb:7000
            - DOWNTIMEDB_URL=http://downtime:7500
            - ELASTICSEARCH_URL=
            - CORS_ALLOW_CREDENTIALS=true
            - CORS_ORIGIN_WHITELIST=http://localhost:8080,http://127.0.0.1:8080
            - CSRF_TRUSTED_ORIGINS=http://localhost:8080,http://127.0.0.1:8080,http://*
            - DRAMATIQ_BROKER_HOST=redis
            - DRAMATIQ_BROKER_PORT=6379
            - OAUTH_SERVER_KEY=mysecretserverkey12345
            - OAUTH_CLIENT_APPS_BASE_URLS=http://configdb:7000,http://downtime:7500,http://science_archive:9500
        mem_limit: "512m"
        restart: always
        command: >
            sh -c "python manage.py migrate 
            && python manage.py create_user -u test_user -p test_pass --superuser --token=sutoken1234abcd
            && python manage.py create_application -u test_user -n ConfigDB --client-id configdb_application_client_id --client-secret configdb_application_client_secret --redirect-uris http://localhost:7000
            && python manage.py create_application -u test_user -n Downtime --client-id downtime_application_client_id --client-secret downtime_application_client_secret --redirect-uris http://localhost:7500/
            && python manage.py create_application -u test_user -n Archive --client-id science_archive_application_client_id --client-secret science_archive_application_client_secret --redirect-uris http://localhost:9500/
            && python manage.py create_semester --id TestSemester
            && python manage.py create_proposal --id TestProposal --active --direct --pi test_user --time-allocation
            && python manage.py create_example_requests -p TestProposal -s test_user
            && python manage.py collectstatic --no-input
            && python manage.py runserver 0.0.0.0:8000"
        depends_on:
            db:
                condition: service_healthy
            configdb:
                condition: service_healthy

    dramatiq_task_scheduler:
        image: ghcr.io/observatorycontrolsystem/observation-portal:4.8.0
        platform: linux/x86_64
        environment:
            - DB_HOST=db
            - DB_NAME=observationportal
            - DB_USER=postgres
            - DB_PASSWORD=postgrespass
            - DEBUG=true
            - SECRET_KEY=ocs_example_obs_portal_secret_key
            - CONFIGDB_URL=http://configdb:7000
            - DOWNTIMEDB_URL=http://downtime:7500
            - dramatiq_prom_db=/tmp
            - ELASTICSEARCH_URL=
            - CORS_ALLOW_CREDENTIALS=true
            - CORS_ORIGIN_WHITELIST=http://localhost:8080,http://127.0.0.1:8080,http://*
            - CSRF_TRUSTED_ORIGINS=http://localhost:8080,http://127.0.0.1:8080,http://*
            - DRAMATIQ_BROKER_HOST=redis
            - DRAMATIQ_BROKER_PORT=6379
            - OAUTH_SERVER_KEY=mysecretserverkey12345
            - OAUTH_CLIENT_APPS_BASE_URLS=http://configdb:7000,http://downtime:7500,http://sciencearchive:9500
        mem_limit: "512m"
        restart: always
        command: >
            sh -c "python manage.py runscript observation_portal.task_scheduler"
        depends_on:
            db:
                condition: service_healthy
            configdb:
                condition: service_healthy
            redis:
                condition: service_healthy

    dramatiq_worker:
        image: ghcr.io/observatorycontrolsystem/observation-portal:4.8.0
        platform: linux/x86_64
        environment:
            - DB_HOST=db
            - DB_NAME=observationportal
            - DB_USER=postgres
            - DB_PASSWORD=postgrespass
            - DEBUG=true
            - SECRET_KEY=ocs_example_obs_portal_secret_key
            - CONFIGDB_URL=http://configdb:7000
            - DOWNTIMEDB_URL=http://downtime:7500
            - dramatiq_prom_db=/tmp
            - ELASTICSEARCH_URL=
            - CORS_ALLOW_CREDENTIALS=true
            - CORS_ORIGIN_WHITELIST=http://localhost:8080,http://127.0.0.1:8080
            - CSRF_TRUSTED_ORIGINS=http://localhost:8080,http://127.0.0.1:8080,http://*
            - DRAMATIQ_BROKER_HOST=redis
            - DRAMATIQ_BROKER_PORT=6379
            - OAUTH_SERVER_KEY=mysecretserverkey12345
            - OAUTH_CLIENT_APPS_BASE_URLS=http://configdb:7000,http://downtime:7500,http://sciencearchive:9500
        mem_limit: "512m"
        restart: always
        command: >
            sh -c "python manage.py rundramatiq --processes 2 --threads 4"
        depends_on:
            db:
                condition: service_healthy
            configdb:
                condition: service_healthy
            redis:
                condition: service_healthy

    adaptive_scheduler:
        image: ghcr.io/observatorycontrolsystem/adaptive_scheduler:v1.5.2
        platform: linux/x86_64
        restart: always
        links:
            -  redis:rediscache
        environment:
            -  OPENTSDB_HOSTNAME=opentsdb-path
            -  OPENTSDB_PYTHON_METRICS_TEST_MODE=True
            -  OBSERVATION_PORTAL_URL=http://observation-portal:8000
            -  OBSERVATION_PORTAL_API_TOKEN=sutoken1234abcd
            -  CONFIGDB_URL=http://configdb:7000
            -  DOWNTIMEDB_URL=http://downtime:7500/
            -  REDIS_URL=redis://rediscache/2
            -  TELESCOPE_CLASSES=1m0,2m0
            -  SAVE_PICKLE_INPUT_FILES=False
            -  SAVE_JSON_OUTPUT_FILES=False
            -  TIME_BETWEEN_RUNS=300
            -  KERNEL_TIMELIMIT=1200
            -  MODEL_HORIZON=2
            -  MODEL_SLICESIZE=300
            -  NO_WEATHER=True
            -  KERNEL_ALGORITHM=SCIP
        volumes:
            # Edit these volume maps to wherever you want to store the log files and input/output data sets
            -  ./data/input:/app/data/input_states/
            -  ./data/output:/app/data/output_schedule/
            -  ./data/:/app/data/
            -  ./logs/:/app/logs/
        command: adaptive-scheduler
        depends_on:
            db:
                condition: service_healthy
            configdb:
                condition: service_healthy
            redis:
                condition: service_healthy

    ocs_frontend:
        image: ghcr.io/observatorycontrolsystem/ocs-example-frontend:0.1.1
        platform: linux/x86_64
        ports:
            - "8080:8080"
        restart: always
        environment:
            -  VUE_APP_OBSERVATION_PORTAL_API_URL=http://127.0.0.1:8000
            -  INTERNAL_OBSERVATION_PORTAL_API_URL=http://127.0.0.1:8000
        entrypoint: /entrypoint.sh

    # The reason for using four minio servers:
    # The science archive requires an "s3" bucket with versioning enabled.
    # In order to use versioning, minio requires a distributed erasure code setup.
    # Distributed minio setups require at least four servers.
    # For details, see: https://docs.min.io/docs/minio-erasure-code-quickstart-guide.html
    minio1:
        image: minio/minio:RELEASE.2021-06-17T00-10-46Z
        volumes:
            - ./data/data1-1:/data1
            - ./data/data1-2:/data2
        expose:
            - "9000"
        environment:
            MINIO_ROOT_USER: minio_access_key
            MINIO_ROOT_PASSWORD: minio_secret
            MINIO_REGION_NAME: minio-region
            MINIO_DOMAIN: localhost
        command: server http://minio{1...4}/data{1...2}
        healthcheck:
            test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
            interval: 30s
            timeout: 20s
            retries: 3
        networks:
          default:
            aliases:
              -  ocs-example-bucket.localhost
    minio2:
        image: minio/minio:RELEASE.2021-06-17T00-10-46Z
        volumes:
            - ./data/data2-1:/data1
            - ./data/data2-2:/data2
        expose:
            - "9000"
        environment:
            MINIO_ROOT_USER: minio_access_key
            MINIO_ROOT_PASSWORD: minio_secret
            MINIO_REGION_NAME: minio-region
            MINIO_DOMAIN: localhost
        command: server http://minio{1...4}/data{1...2}
        healthcheck:
            test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
            interval: 30s
            timeout: 20s
            retries: 3
        networks:
          default:
            aliases:
              -  ocs-example-bucket.localhost
    minio3:
        image: minio/minio:RELEASE.2021-06-17T00-10-46Z
        volumes:
            - ./data/data3-1:/data1
            - ./data/data3-2:/data2
        expose:
            - "9000"
        environment:
            MINIO_ROOT_USER: minio_access_key
            MINIO_ROOT_PASSWORD: minio_secret
            MINIO_REGION_NAME: minio-region
            MINIO_DOMAIN: localhost
        command: server http://minio{1...4}/data{1...2}
        healthcheck:
            test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
            interval: 30s
            timeout: 20s
            retries: 3
        networks:
          default:
            aliases:
              -  ocs-example-bucket.localhost
    minio4:
        image: minio/minio:RELEASE.2021-06-17T00-10-46Z
        volumes:
            - ./data/data4-1:/data1
            - ./data/data4-2:/data2
        expose:
            - "9000"
        environment:
            MINIO_ROOT_USER: minio_access_key
            MINIO_ROOT_PASSWORD: minio_secret
            MINIO_REGION_NAME: minio-region
            MINIO_DOMAIN: localhost
        command: server http://minio{1...4}/data{1...2}
        healthcheck:
            test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
            interval: 30s
            timeout: 20s
            retries: 3
        networks:
          default:
            aliases:
              -  ocs-example-bucket.localhost

    nginx:
        image: observatorycontrolsystem/nginx-mod-zip:1.15.12
        platform: linux/x86_64
        volumes:
        - ./nginx.conf:/etc/nginx/nginx.conf:ro
        network_mode: "service:science_archive"
        depends_on:
        - minio1
        - minio2
        - minio3
        - minio4

    science_archive:
        image: ghcr.io/observatorycontrolsystem/science-archive:2.3.6
        platform: linux/x86_64
        container_name: sciencearchive
        links:
            -  redis:rediscache
        ports:
            - "9000:9000"  # minio
            - "9500:9500"  # archive api
        expose:
            - "9501"
        environment:
            - DEBUG=True  # must be true to load static files
            - DB_HOST=db
            - DB_HOST_READER=db
            - DB_NAME=sciencearchive
            - DB_USER=postgres
            - DB_PASS=postgrespass
            - SECRET_KEY=ocs_example_science_archive_secret_key
            - FILESTORE_TYPE=s3
            - AWS_ACCESS_KEY_ID=minio_access_key
            - AWS_SECRET_ACCESS_KEY=minio_secret
            - AWS_DEFAULT_REGION=minio-region
            - AWS_BUCKET=ocs-example-bucket
            - BUCKET=ocs-example-bucket
            - S3_ENDPOINT_URL=http://localhost:9000/
            - OAUTH_CLIENT_ID=science_archive_application_client_id
            - OAUTH_CLIENT_SECRET=science_archive_application_client_secret
            - OAUTH_TOKEN_URL=http://observation-portal:8000/o/token/
            - OAUTH_PROFILE_URL=http://observation-portal:8000/api/profile/
            - OAUTH_SERVER_KEY=mysecretserverkey12345
            - CONFIGDB_URL=http://configdb:7000/
            - NAVBAR_TITLE_URL=http://localhost:9500/
            - PROCESSED_EXCHANGE_ENABLED=False
            - CACHE_BACKEND=django_redis.cache.RedisCache
            - CACHE_LOC=redis://rediscache:6379/3
            # Additional variables for the ingester:
            - API_ROOT=http://localhost:9500/
            - OBSERVATION_PORTAL_BASE_URL=http://observation-portal:8000/api
            - OPENTSDB_PYTHON_METRICS_TEST_MODE=True
        mem_limit: "512m"
        restart: always
        volumes: 
            - ./example_data:/example_data
            - ./scripts/science_archive:/scripts
        healthcheck:
            test: ["CMD-SHELL", "wget localhost:9500/ -q -O - > /dev/null 2>&1"]
            interval: 10s
            timeout: 5s
            retries: 5
        command: >
            sh -c "python manage.py migrate ;
            python manage.py collectstatic --no-input ;
            python manage.py runserver 0.0.0.0:9501 &
            pip install requests ocs_ingester;
            sleep 2 ;
            python /scripts/initialize_bucket.py ;
            python /scripts/ingest_sample_data.py ;
            python manage.py cacheaggregates ;
            wait %1"
        depends_on:
        - observation-portal
        - minio1
        - minio2
        - minio3
        - minio4

    science_archive_client:
        image: ghcr.io/observatorycontrolsystem/science-archive-client:1.1.1
        platform: linux/x86_64
        ports:
         - "9600:8080"
        restart: always
        environment:
         - VUE_APP_OBSERVATION_PORTAL_API_URL=http://127.0.0.1:8000/api
         - VUE_APP_ARCHIVE_API_URL=http://127.0.0.1:9500
         - VUE_APP_SIMBAD_SERVICE_URL=http://127.0.0.1:9800
         - VUE_APP_THUMBNAIL_SERVICE_URL=http://127.0.0.1:9700
         - VUE_APP_DOCUMENTATION_URL=https://lco.global/documentation/archive-documentation/
         - VUE_APP_GITHUB_API_URL=https://github.com/observatorycontrolsystem/science-archive
         - "VUE_APP_REDUCTION_LEVEL_OPTIONS={\"0\": \"Raw\", \"91\": \"Processed\"}"
        depends_on:
         - science_archive

    thumbnail_service:
        image: ghcr.io/observatorycontrolsystem/thumbnail-service:3.0.2
        platform: linux/x86_64
        container_name: thumbnailservice
        ports:
         - 9700:8080
        environment:
         - ARCHIVE_API_URL=http://sciencearchive:9500/
         - STORAGE_URL=http://localhost:9000/
         - AWS_ACCESS_KEY_ID=minio_access_key
         - AWS_SECRET_ACCESS_KEY=minio_secret
         - AWS_DEFAULT_REGION=minio-region
         - AWS_BUCKET=ocs-example-bucket
         - TMP_DIR=/thumbnails/
        restart: always
        volumes: 
            - ./data/thumbnails:/thumbnails
        command: >
            sh -c "gunicorn
            --config=thumbservice/config.py
            --bind=0.0.0.0:8080
            --worker-class=gevent
            --access-logfile=-
            --error-logfile=-
            thumbservice.thumbservice:app"
        depends_on:
         - science_archive

    simbad2k:
        image: ghcr.io/observatorycontrolsystem/simbad2k:1.2.0
        platform: linux/x86_64
        container_name: simbad2k
        ports:
         - 9800:5000
        environment:
         - XDG_CACHE_HOME=/tmp/astropy-cache
         - XDG_CONFIG_HOME=/tmp/astropy-config
        depends_on:
         - science_archive

    redis:
        image: redis:6.2.6
        command: ["redis-server", "--appendonly", "yes"]
        restart: always
        ports:
            -  6373:6379
        volumes:
            -  ./data/redis:/data
        healthcheck:
            test: ["CMD", "redis-cli", "ping"]
            interval: 10s
            timeout: 5s
            retries: 30

